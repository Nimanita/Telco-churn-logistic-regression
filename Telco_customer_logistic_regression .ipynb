{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0087f4e-98d4-42e9-bbb5-eab222f9edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math , copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd7de1-1400-4537-9163-3da0f182dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Telco-customer-churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8064d95-fee4-4c8e-aefc-1f595e66a13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69dbe0-28ea-4245-a4e4-858fa625837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the engineer_features function here\n",
    "def engineer_features(df):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # Convert TotalCharges to numeric\n",
    "    df_eng['TotalCharges'] = pd.to_numeric(df_eng['TotalCharges'], errors='coerce')\n",
    "    df_eng['TotalCharges'] = df_eng['TotalCharges'].fillna(0)\n",
    "    \n",
    "    # Create interaction terms\n",
    "    # Tenure and contract type interaction\n",
    "    df_eng['tenure_month_to_month'] = df_eng['tenure'] * (df_eng['Contract'] == 'Month-to-month').astype(int)\n",
    "    df_eng['tenure_one_year'] = df_eng['tenure'] * (df_eng['Contract'] == 'One year').astype(int)\n",
    "    df_eng['tenure_two_year'] = df_eng['tenure'] * (df_eng['Contract'] == 'Two year').astype(int)\n",
    "    \n",
    "    # Monthly charges and tenure interaction\n",
    "    df_eng['charge_per_tenure'] = df_eng['MonthlyCharges'] / (df_eng['tenure'] + 1)  # +1 to avoid division by zero\n",
    "    \n",
    "    # Create ratio features\n",
    "    df_eng['avg_monthly_spend'] = df_eng['TotalCharges'] / (df_eng['tenure'] + 1)\n",
    "    df_eng['recent_vs_lifetime_spend'] = df_eng['MonthlyCharges'] / (df_eng['TotalCharges'] + 1)\n",
    "    \n",
    "    # Service aggregation feature\n",
    "    df_eng['total_services'] = (\n",
    "        (df_eng['PhoneService'] == 'Yes').astype(int) +\n",
    "        (df_eng['MultipleLines'] == 'Yes').astype(int) +\n",
    "        (df_eng['InternetService'] != 'No').astype(int) +\n",
    "        (df_eng['OnlineSecurity'] == 'Yes').astype(int) +\n",
    "        (df_eng['OnlineBackup'] == 'Yes').astype(int) +\n",
    "        (df_eng['DeviceProtection'] == 'Yes').astype(int) +\n",
    "        (df_eng['TechSupport'] == 'Yes').astype(int) +\n",
    "        (df_eng['StreamingTV'] == 'Yes').astype(int) +\n",
    "        (df_eng['StreamingMovies'] == 'Yes').astype(int)\n",
    "    )\n",
    "    \n",
    "    # Customer loyalty feature - combines tenure with contract type\n",
    "    contract_value = df_eng['Contract'].map({'Month-to-month': 1, 'One year': 12, 'Two year': 24})\n",
    "    df_eng['loyalty_score'] = df_eng['tenure'] * contract_value\n",
    "    \n",
    "    return df_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4062693-50b0-4a17-b6ac-926d53885590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = engineer_features(df)\n",
    "print(\"Data shape after feature engineering:\", df.shape)\n",
    "print(\"New features added:\", [col for col in df.columns if col not in pd.read_csv(\"Telco-customer-churn.csv\").columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d368d5-5394-4fca-9326-8c00a3b5d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"\\nData types and missing values:\")\n",
    "missing_data = pd.DataFrame({\n",
    "    'Data Type': df.dtypes,\n",
    "    'Missing Values': df.isnull().sum(),\n",
    "    'Missing Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "})\n",
    "print(missing_data)\n",
    "\n",
    "# Check unique values in each column\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object' or len(df[col].unique()) < 10:\n",
    "        print(f\"\\nUnique values in {col}:\")\n",
    "        print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae39dac-1d39-4521-b7dd-c18396408775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plots\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.suptitle('Customer Churn Analysis', fontsize=20)\n",
    "\n",
    "# 1. Distribution of churn\n",
    "plt.subplot(2, 3, 1)\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "plt.pie(churn_counts, labels=['No', 'Yes'], autopct='%1.1f%%', startangle=90, colors=['lightblue', 'coral'])\n",
    "plt.title('Distribution of Churn')\n",
    "\n",
    "# 2. Tenure vs Churn\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.boxplot(x='Churn', y='tenure', data=df)\n",
    "plt.title('Tenure vs Churn')\n",
    "\n",
    "# 3. Monthly Charges vs Churn\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.boxplot(x='Churn', y='MonthlyCharges', data=df)\n",
    "plt.title('Monthly Charges vs Churn')\n",
    "\n",
    "# 4. Contract Type vs Churn\n",
    "plt.subplot(2, 3, 4)\n",
    "contract_churn = pd.crosstab(df['Contract'], df['Churn'])\n",
    "contract_churn_pct = contract_churn.div(contract_churn.sum(axis=1), axis=0) * 100\n",
    "contract_churn_pct['Yes'].plot(kind='bar', color='coral')\n",
    "plt.title('Churn Rate by Contract Type')\n",
    "plt.ylabel('Churn Rate (%)')\n",
    "\n",
    "# 5. Payment Method vs Churn\n",
    "plt.subplot(2, 3, 5)\n",
    "payment_churn = pd.crosstab(df['PaymentMethod'], df['Churn'])\n",
    "payment_churn_pct = payment_churn.div(payment_churn.sum(axis=1), axis=0) * 100\n",
    "payment_churn_pct['Yes'].plot(kind='bar', color='coral')\n",
    "plt.title('Churn Rate by Payment Method')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Churn Rate (%)')\n",
    "\n",
    "# 6. Correlation matrix of numerical variables\n",
    "plt.subplot(2, 3, 6)\n",
    "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "df[numerical_cols] = df[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "correlation = df[numerical_cols + ['SeniorCitizen']].corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fb5a8-1910-48fd-b8c0-7afefd53500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b66f7d-6d06-416a-95ee-16204a35813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing data\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_columns.remove('customerID') \n",
    "#categorical_columns.remove('TotalCharges')# Remove identifier\n",
    "if 'Churn' in categorical_columns:\n",
    "    categorical_columns.remove('Churn')  # Remove target variable for now\n",
    "\n",
    "#print(f\"\\nCategorical columns to encode: {categorical_columns}\")\n",
    "\n",
    "# Initialize the encoder for all categorical columns\n",
    "full_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "# Fit and transform all categorical columns\n",
    "encoded_cats = full_encoder.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Get feature names\n",
    "encoded_feature_names = full_encoder.get_feature_names_out(categorical_columns)\n",
    "#print(f\"\\nNumber of features after encoding: {len(encoded_feature_names)}\")\n",
    "#print(\"First 10 encoded features:\", encoded_feature_names[:10])\n",
    "\n",
    "# Create DataFrame with encoded features\n",
    "encoded_df = pd.DataFrame(encoded_cats, columns=encoded_feature_names)\n",
    "\n",
    "# Fix TotalCharges column - convert to numeric\n",
    "#df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "#df['TotalCharges'] = df['TotalCharges'].fillna(0)\n",
    "# Get numerical columns (excluding customerID)\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if 'customerID' in numerical_columns:\n",
    "    numerical_columns.remove('customerID')\n",
    "\n",
    "print(f\"\\nNumerical columns: {numerical_columns}\")\n",
    "if 'customerID' in numerical_columns:\n",
    "    numerical_columns.remove('customerID')\n",
    "\n",
    "#print(f\"\\nNumerical columns: {numerical_columns}\")\n",
    "\n",
    "# Copy numerical data\n",
    "numerical_df = df[numerical_columns].reset_index(drop=True)\n",
    "\n",
    "# Convert target variable to numeric (0/1)\n",
    "churn_numeric = df['Churn'].map({'Yes': 1, 'No': 0}).reset_index(drop=True)\n",
    "\n",
    "final_df = pd.concat([numerical_df, encoded_df, churn_numeric], axis=1)\n",
    "\n",
    "print(\"\\nFinal dataset after one-hot encoding:\")\n",
    "print(f\"Shape: {final_df.shape}\")\n",
    "print(\"First 5 rows:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24504f99-e4aa-4a65-8934-685c760493f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(x_train , return_params=False):\n",
    "    \"\"\"\n",
    "    Perform feature scaling using standardization (Z-score normalization)\n",
    "    \n",
    "    Parameters:\n",
    "    x_train (numpy.ndarray): Input feature matrix\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Scaled feature matrix\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original array\n",
    "          \n",
    "    scaled_x_train = x_train.copy().astype('float')\n",
    "    scaling_columns = ['tenure', 'MonthlyCharges', 'TotalCharges' , 'tenure_month_to_month','charge_per_tenure',  'avg_monthly_spend',\n",
    "                      'tenure_one_year', 'tenure_two_year', 'recent_vs_lifetime_spend' , 'total_services' , 'loyalty_score']\n",
    "    #scaling_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "    params = {'mean': {}, 'std': {}}\n",
    "   \n",
    "    # Scale each column (feature) separately\n",
    "    for col in scaling_columns:\n",
    "        mean = x_train[col].mean()\n",
    "        std = x_train[col].std()\n",
    "        params['mean'][col] = mean\n",
    "        params['std'][col] = std\n",
    "        \n",
    "        # Apply z-score normalization with handling for zero std\n",
    "        if std == 0:\n",
    "            scaled_x_train[col] = 0\n",
    "        else:\n",
    "            scaled_x_train[col] = (x_train[col] - mean) / std\n",
    "            \n",
    "    if return_params:\n",
    "        return scaled_x_train, params\n",
    "    else:\n",
    "        return scaled_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b6d4d-86ac-4ab1-bd68-42dbc8b1756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#VERIFICATION OF PREPROCESSING PROCESS\n",
    "numeric_columns = final_df.select_dtypes(include=['int64' , 'float64']).columns.tolist()\n",
    "print(numeric_columns)\n",
    "print(\"\\nData types and missing values:\")\n",
    "missing_data = pd.DataFrame({\n",
    "    'Data Type': final_df.dtypes,\n",
    "    'Missing Values': final_df.isnull().sum(),\n",
    "    'Missing Percentage': (final_df.isnull().sum() / len(final_df)) * 100\n",
    "})\n",
    "print(missing_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9309ad-cb6c-445f-a64f-0e5df13da60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_prediction(x, w, b):\n",
    "    \"\"\"\n",
    "    Vectorized sigmoid activation function\n",
    "    Args:\n",
    "        x: input features (m, n) or single example (n,)\n",
    "        w: weights (n,)\n",
    "        b: bias (scalar)\n",
    "    Returns:\n",
    "        sigmoid(z) where z = xÂ·w + b\n",
    "    \"\"\"\n",
    "    z = np.dot(x, w) + b\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476598d0-09d9-4087-9dd0-5256dea7461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x,w,b,y):\n",
    "    m = x.shape[0]\n",
    "    sum = 0\n",
    "    predictions = compute_model_prediction(x, w, b)\n",
    "    cost = -1/m * np.sum(y*np.log(predictions) + (1-y)*np.log(1-predictions))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b0bfe-cbc5-41b7-ba6c-f9a4b4479e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_descent(x_train , y_train , w , b):\n",
    "    m , n = x_train.shape\n",
    "    dw = np.zeros((n,)) \n",
    "    db = 0\n",
    "   \n",
    "    model_prediction = compute_model_prediction(x_train, w, b)\n",
    "    diff = (model_prediction - y_train)\n",
    "    \n",
    "    # Vectorized computation of gradients\n",
    "    dw_dj = (1/m) * np.dot(x_train.T, diff)\n",
    "    db_dj = (1/m) * np.sum(diff)\n",
    "    \n",
    "    return dw_dj , db_dj\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa8c07-96c7-47a4-bffa-849d0345295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x_train , y_train , w , b , alpha_in , iteration):\n",
    "    m , n = x_train.shape\n",
    " \n",
    "    alpha = alpha_in\n",
    "    cost = compute_cost(x_train , w ,b , y_train)\n",
    "    costs = []\n",
    "    iterations = []\n",
    "    w1_range = []\n",
    "    w2_range = []\n",
    "    w3_range = []\n",
    "    for i in range(iteration):\n",
    " \n",
    "        dw_dj , db_dj = compute_gradient_descent(x_train , y_train , w , b)\n",
    "        w = w - alpha * dw_dj\n",
    "        b = b - alpha * db_dj\n",
    "        cost = compute_cost(x_train , w ,b , y_train)\n",
    "        costs.append(cost)\n",
    "        iterations.append(i)\n",
    "        #print(w , w[0])\n",
    "        #w1_range.append(w[0])\n",
    "        #w2_range.append(w[1])\n",
    "        #print(w , \" \" , b , \" \" , cost)\n",
    "        \n",
    "    return w , b , costs , iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3b41c-784b-4b89-bdbb-91cdaec65e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(x, w, b, y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost with L2 regularization\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    predictions = compute_model_prediction(x, w, b)\n",
    "    \n",
    "    # Compute regular cost\n",
    "    regular_cost = -1/m * np.sum(y*np.log(predictions) + (1-y)*np.log(1-predictions))\n",
    "    \n",
    "    # Add regularization term (excluding bias)\n",
    "    reg_cost = (lambda_/(2*m)) * np.sum(w**2)\n",
    "    \n",
    "    return regular_cost + reg_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979bd633-5131-4c91-9d3c-ae446539078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_with_regularization(x_train, y_train, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Compute gradient with L2 regularization\n",
    "    \"\"\"\n",
    "    m, n = x_train.shape\n",
    "    \n",
    "    model_prediction = compute_model_prediction(x_train, w, b)\n",
    "    diff = (model_prediction - y_train)\n",
    "    \n",
    "    # Gradient of cost without regularization\n",
    "    dw_j = (1/m) * np.dot(x_train.T, diff)\n",
    "    db_j = (1/m) * np.sum(diff)\n",
    "    \n",
    "    # Add regularization term to gradient (not to bias)\n",
    "    dw_j = dw_j + (lambda_/m) * w\n",
    "    \n",
    "    return dw_j, db_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79bff7-6b47-4b2f-b65b-6c9c6c81bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_regularization(x_train, y_train, w, b, alpha_in, iteration, lambda_):\n",
    "    \"\"\"\n",
    "    Gradient descent with regularization\n",
    "    \"\"\"\n",
    "    m, n = x_train.shape\n",
    "    alpha = alpha_in\n",
    "    cost = compute_cost_with_regularization(x_train, w, b, y_train, lambda_)\n",
    "    costs = []\n",
    "    iterations = []\n",
    "    \n",
    "    for i in range(iteration):\n",
    "        dw_j, db_j = compute_gradient_with_regularization(x_train, y_train, w, b, lambda_)\n",
    "        w = w - alpha * dw_j\n",
    "        b = b - alpha * db_j\n",
    "        \n",
    "        \n",
    "        cost = compute_cost_with_regularization(x_train, w, b, y_train, lambda_)\n",
    "        costs.append(cost)\n",
    "        iterations.append(i)\n",
    "            \n",
    "    return w, b, costs, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd59c28-0dae-4dc7-a775-0320197f8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_vs_iteration(iterations, costs):\n",
    "    \"\"\"\n",
    "    Plot a graph of cost versus iteration.\n",
    "    \n",
    "    Parameters:\n",
    "    iterations (list or numpy array): Number of iterations\n",
    "    costs (list or numpy array): Corresponding cost values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(iterations, costs, marker='o')\n",
    "    plt.title('Cost vs Iteration', fontsize=16)\n",
    "    plt.xlabel('Iteration', fontsize=12)\n",
    "    plt.ylabel('Cost', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25a646-b544-4550-96c9-f5ed23c142b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_model(X, y_true, w, b):\n",
    "    \"\"\"\n",
    "    Evaluate the binary classification model\n",
    "    \n",
    "    Parameters:\n",
    "    X: Features\n",
    "    y_true: True labels\n",
    "    w: Weights\n",
    "    b: Bias\n",
    "    \n",
    "    Returns:\n",
    "    accuracy, precision, recall, f1_score\n",
    "    \"\"\"\n",
    "    # Get predictions (probabilities)\n",
    "    y_prob = compute_model_prediction(X, w, b)\n",
    "    \n",
    "    # Convert to binary predictions using 0.5 threshold\n",
    "    y_pred = (y_prob >= 0.25).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    predicted_positives = np.sum(y_pred == 1)\n",
    "    actual_positives = np.sum(y_true == 1)\n",
    "    \n",
    "    precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "     \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590eeaa-ab2d-40f7-9887-f44b380a9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_vs_actual(y_actual, y_pred, title):\n",
    "    \"\"\"\n",
    "    Plot predicted values against actual values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_actual, y_pred, alpha=0.5)\n",
    "    \n",
    "    # Add a perfect prediction line\n",
    "    min_val = min(np.min(y_actual), np.min(y_pred))\n",
    "    max_val = max(np.max(y_actual), np.max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fff6d8-5999-47aa-8326-a991cc5ee816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for binary classification\n",
    "    \n",
    "    Parameters:\n",
    "    y_true: True labels\n",
    "    y_pred: Predicted labels\n",
    "    title: Plot title\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = np.zeros((2, 2))\n",
    "    cm[0, 0] = np.sum((y_pred == 0) & (y_true == 0))  # True Negatives\n",
    "    cm[0, 1] = np.sum((y_pred == 0) & (y_true == 1))  # False Negatives\n",
    "    cm[1, 0] = np.sum((y_pred == 1) & (y_true == 0))  # False Positives\n",
    "    cm[1, 1] = np.sum((y_pred == 1) & (y_true == 1))  # True Positives\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['Not Churned (0)', 'Churned (1)']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, f\"{int(cm[i, j])}\", \n",
    "                     horizontalalignment=\"center\", \n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15fb169-33e6-4eb0-97fd-a176861e1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_score, title):\n",
    "    \"\"\"\n",
    "    Plot ROC curve with correct AUC calculation\n",
    "    \n",
    "    Parameters:\n",
    "    y_true: True labels\n",
    "    y_score: Predicted probabilities\n",
    "    title: Plot title\n",
    "    \"\"\"\n",
    "    # Get unique threshold values (sorted predicted probabilities)\n",
    "    thresholds = np.sort(np.unique(y_score))\n",
    "    # Add 0 and 1 to ensure full curve\n",
    "    thresholds = np.append(thresholds, [0, 1])\n",
    "    thresholds = np.sort(thresholds)\n",
    "    \n",
    "    n_thresholds = len(thresholds)\n",
    "    tpr = np.zeros(n_thresholds)\n",
    "    fpr = np.zeros(n_thresholds)\n",
    "    \n",
    "    # Calculate TPR and FPR for each threshold\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        y_pred = (y_score >= threshold).astype(int)\n",
    "        \n",
    "        # True positives and false positives\n",
    "        tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        \n",
    "        # True negatives and false negatives\n",
    "        tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "        fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "        \n",
    "        # TPR and FPR\n",
    "        tpr[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr[i] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    \n",
    "    # Sort by increasing FPR for proper curve\n",
    "    sorted_indices = np.argsort(fpr)\n",
    "    fpr_sorted = fpr[sorted_indices]\n",
    "    tpr_sorted = tpr[sorted_indices]\n",
    "    \n",
    "    # Calculate AUC using trapezoidal rule\n",
    "    auc = np.trapz(tpr_sorted, fpr_sorted)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_sorted, tpr_sorted, 'b-', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'r--')  # Diagonal line\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title} (AUC = {auc:.4f})')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eadfa5a-44a4-4d44-a6a0-4f69db78dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(X, weights, n_top=20):\n",
    "    \"\"\"\n",
    "    Plot feature importance based on the magnitude of weights\n",
    "    \n",
    "    Parameters:\n",
    "    X: DataFrame with feature names\n",
    "    weights: Model weights\n",
    "    n_top: Number of top features to display (default: 20)\n",
    "    \"\"\"\n",
    "    # Get feature names\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Create DataFrame with feature names and weights\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Weight': weights,\n",
    "        'Absolute Weight': np.abs(weights)\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute weight value (importance)\n",
    "    importance_df = importance_df.sort_values('Absolute Weight', ascending=False)\n",
    "  \n",
    "    \n",
    "    sorted_feature_names = importance_df['Feature'].tolist()\n",
    "    \n",
    "    # Clean print in list format: ['feature1', 'feature2', ..., 'featureN']\n",
    "    print(\"\\nSorted Feature Names (Python list format):\")\n",
    "    print(\"[\" + \", \".join(f\"'{col}'\" for col in sorted_feature_names) + \"]\")\n",
    "    # Take top n features\n",
    "    #if n_top is not None:\n",
    "    #    importance_df = importance_df.head(n_top)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['blue' if w > 0 else 'red' for w in importance_df['Weight']]\n",
    "    plt.barh(importance_df['Feature'], importance_df['Absolute Weight'], color=colors)\n",
    "    plt.xlabel('Absolute Weight Magnitude')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance (Weight Magnitude)')\n",
    "    \n",
    "    # Add a legend\n",
    "    import matplotlib.patches as mpatches\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Positive Impact (Decreases Churn)')\n",
    "    red_patch = mpatches.Patch(color='red', label='Negative Impact (Increases Churn)')\n",
    "    plt.legend(handles=[blue_patch, red_patch], loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494546b-2eac-459c-85e2-7b6d1f84d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling 3 column tenure , MonthlyCharges  TotalCharges\n",
    "X = final_df.drop('Churn', axis=1)\n",
    "y = final_df['Churn']\n",
    "\n",
    "# Create training and test sets (80% train, 20% test is common)\n",
    "X_train, X_test, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale only the numerical features in both sets using the parameters from training set\n",
    "X_train_scaled, scaling_params = feature_scaling(X_train, return_params=True)\n",
    "\n",
    "# Apply the same scaling parameters to test set\n",
    "X_val_scaled = X_test.copy().astype('float')\n",
    "for col in ['tenure', 'MonthlyCharges', 'TotalCharges' , 'tenure_month_to_month','charge_per_tenure',  'avg_monthly_spend',\n",
    "                       'tenure_one_year', 'tenure_two_year', 'recent_vs_lifetime_spend']:\n",
    "    X_val_scaled[col] = (X_test[col] - scaling_params['mean'][col]) / scaling_params['std'][col]\n",
    "\n",
    "m, n = X_train_scaled.shape\n",
    "w = np.zeros(n)\n",
    "b = 0\n",
    "print(\"Initial weights and bias:\", w, b)\n",
    "\n",
    "# Train model\n",
    "w, b,  costs, iterations = gradient_descent_with_regularization(X_train_scaled, y_train, w, b, 0.6, 40000, 0.02)\n",
    "print(\"Final weights and bias:\", w, b)\n",
    "\n",
    "# Evaluate on training set\n",
    "train_accuracy, train_precision, train_recall, train_f1 = evaluate_classification_model(X_train_scaled, y_train, w, b)\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1 Score: {train_f1:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy, test_precision, test_recall, test_f1 = evaluate_classification_model(X_val_scaled, y_val, w, b)\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_prob = compute_model_prediction(X_train_scaled, w , b)\n",
    "y_test_pred_prob = compute_model_prediction(X_val_scaled, w , b)\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_train_pred = (y_train_pred_prob >= 0.5).astype(int)\n",
    "y_test_pred = (y_test_pred_prob >= 0.5).astype(int)\n",
    "# Plot predictions\n",
    "plot_cost_vs_iteration(iterations, costs)\n",
    "\n",
    "plot_confusion_matrix(y_train, y_train_pred, 'Training Set: Confusion Matrix')\n",
    "plot_confusion_matrix(y_val, y_test_pred, 'Test Set: Confusion Matrix')\n",
    "# Plot feature importance\n",
    "\n",
    "plot_roc_curve(y_train, y_train_pred_prob, 'Training Set: ROC Curve')\n",
    "plot_roc_curve(y_val, y_test_pred_prob, 'Test Set: ROC Curve')\n",
    "\n",
    "plot_feature_importance(X_train_scaled, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056bb59-cf8b-4dad-b859-2ee1c0ded6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code after your existing evaluation\n",
    "print(\"\\n--- Testing different thresholds ---\")\n",
    "\n",
    "# Test different threshold values\n",
    "thresholds = [0.25, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold to predictions\n",
    "    y_train_pred_threshold = (y_train_pred_prob >= threshold).astype(int)\n",
    "    y_test_pred_threshold = (y_test_pred_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics for training set\n",
    "    train_tp = np.sum((y_train_pred_threshold == 1) & (y_train == 1))\n",
    "    train_fp = np.sum((y_train_pred_threshold == 1) & (y_train == 0))\n",
    "    train_fn = np.sum((y_train_pred_threshold == 0) & (y_train == 1))\n",
    "    train_precision = train_tp / (train_tp + train_fp) if (train_tp + train_fp) > 0 else 0\n",
    "    train_recall = train_tp / (train_tp + train_fn) if (train_tp + train_fn) > 0 else 0\n",
    "    train_f1 = 2 * train_precision * train_recall / (train_precision + train_recall) if (train_precision + train_recall) > 0 else 0\n",
    "    \n",
    "    # Calculate metrics for test set\n",
    "    test_tp = np.sum((y_test_pred_threshold == 1) & (y_val == 1))\n",
    "    test_fp = np.sum((y_test_pred_threshold == 1) & (y_val == 0))\n",
    "    test_fn = np.sum((y_test_pred_threshold == 0) & (y_val == 1))\n",
    "    test_precision = test_tp / (test_tp + test_fp) if (test_tp + test_fp) > 0 else 0\n",
    "    test_recall = test_tp / (test_tp + test_fn) if (test_tp + test_fn) > 0 else 0\n",
    "    test_f1 = 2 * test_precision * test_recall / (test_precision + test_recall) if (test_precision + test_recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nThreshold = {threshold}\")\n",
    "    print(f\"Train: Precision = {train_precision:.4f}, Recall = {train_recall:.4f}, F1 = {train_f1:.4f}\")\n",
    "    print(f\"Test: Precision = {test_precision:.4f}, Recall = {test_recall:.4f}, F1 = {test_f1:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix for test set\n",
    "''' plt.figure(figsize=(8, 6))\n",
    "    cm = np.zeros((2, 2))\n",
    "    cm[0, 0] = np.sum((y_test_pred_threshold == 0) & (y_test == 0))  # TN\n",
    "    cm[0, 1] = test_fn  # FN\n",
    "    cm[1, 0] = test_fp  # FP\n",
    "    cm[1, 1] = test_tp  # TP\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix (Threshold = {threshold})', fontsize=16)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['Not Churned (0)', 'Churned (1)']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, f\"{int(cm[i, j])}\", \n",
    "                     horizontalalignment=\"center\", \n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9586dc4-ff0b-4c4c-b121-2dec0e20dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling 3 column tenure , MonthlyCharges  TotalCharges\n",
    "X = final_df.drop('Churn', axis=1)\n",
    "y = final_df['Churn']\n",
    "\n",
    "# Create training and test sets (80% train, 20% test is common)\n",
    "X_train, X_test, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale only the numerical features in both sets using the parameters from training set\n",
    "X_train_scaled, scaling_params = feature_scaling(X_train, return_params=True)\n",
    "\n",
    "# Apply the same scaling parameters to test set\n",
    "X_val_scaled = X_test.copy().astype('float')\n",
    "for col in ['tenure', 'MonthlyCharges', 'TotalCharges' , 'tenure_month_to_month','charge_per_tenure',  'avg_monthly_spend',\n",
    "                       'tenure_one_year', 'tenure_two_year', 'recent_vs_lifetime_spend']:\n",
    "    X_val_scaled[col] = (X_test[col] - scaling_params['mean'][col]) / scaling_params['std'][col]\n",
    "\n",
    "noisy_cols = ['StreamingTV_No internet service', 'DeviceProtection_No internet service', \n",
    " 'TechSupport_No internet service', 'StreamingMovies_No internet service', 'InternetService_No', 'OnlineBackup_No internet service',\n",
    " 'OnlineSecurity_No internet service', 'Dependents_Yes', 'DeviceProtection_Yes', 'tenure_one_year', 'loyalty_score', \n",
    " 'PaymentMethod_Credit card (automatic)', 'Partner_Yes',\n",
    " 'PaymentMethod_Mailed check', 'avg_monthly_spend', 'recent_vs_lifetime_spend', 'tenure_two_year']\n",
    "\n",
    "X_train_scaled = X_train_scaled.drop(columns=noisy_cols, errors='ignore')\n",
    "X_val_scaled = X_val_scaled.drop(columns=noisy_cols, errors='ignore')\n",
    "m, n = X_train_scaled.shape\n",
    "w = np.zeros(n)\n",
    "b = 0\n",
    "print(\"Initial weights and bias:\", w, b)\n",
    "\n",
    "# Train model\n",
    "w, b,  costs, iterations = gradient_descent_with_regularization(X_train_scaled, y_train, w, b, 0.6, 30000, 150)\n",
    "print(\"Final weights and bias:\", w, b)\n",
    "\n",
    "# Evaluate on training set\n",
    "train_accuracy, train_precision, train_recall, train_f1 = evaluate_classification_model(X_train_scaled, y_train, w, b)\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1 Score: {train_f1:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy, test_precision, test_recall, test_f1 = evaluate_classification_model(X_val_scaled, y_val, w, b)\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_prob = compute_model_prediction(X_train_scaled, w , b)\n",
    "y_test_pred_prob = compute_model_prediction(X_val_scaled, w , b)\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_train_pred = (y_train_pred_prob >= 0.5).astype(int)\n",
    "y_test_pred = (y_test_pred_prob >= 0.5).astype(int)\n",
    "# Plot predictions\n",
    "plot_cost_vs_iteration(iterations, costs)\n",
    "\n",
    "plot_confusion_matrix(y_train, y_train_pred, 'Training Set: Confusion Matrix')\n",
    "plot_confusion_matrix(y_val, y_test_pred, 'Test Set: Confusion Matrix')\n",
    "# Plot feature importance\n",
    "\n",
    "plot_roc_curve(y_train, y_train_pred_prob, 'Training Set: ROC Curve')\n",
    "plot_roc_curve(y_val, y_test_pred_prob, 'Test Set: ROC Curve')\n",
    "\n",
    "plot_feature_importance(X_train_scaled, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809983b-669a-4045-87a4-1233bb1781ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code after your existing evaluation\n",
    "print(\"\\n--- Testing different thresholds ---\")\n",
    "\n",
    "# Test different threshold values\n",
    "thresholds = [0.25, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold to predictions\n",
    "    y_train_pred_threshold = (y_train_pred_prob >= threshold).astype(int)\n",
    "    y_test_pred_threshold = (y_test_pred_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics for training set\n",
    "    train_tp = np.sum((y_train_pred_threshold == 1) & (y_train == 1))\n",
    "    train_fp = np.sum((y_train_pred_threshold == 1) & (y_train == 0))\n",
    "    train_fn = np.sum((y_train_pred_threshold == 0) & (y_train == 1))\n",
    "    train_precision = train_tp / (train_tp + train_fp) if (train_tp + train_fp) > 0 else 0\n",
    "    train_recall = train_tp / (train_tp + train_fn) if (train_tp + train_fn) > 0 else 0\n",
    "    train_f1 = 2 * train_precision * train_recall / (train_precision + train_recall) if (train_precision + train_recall) > 0 else 0\n",
    "    \n",
    "    # Calculate metrics for test set\n",
    "    test_tp = np.sum((y_test_pred_threshold == 1) & (y_val == 1))\n",
    "    test_fp = np.sum((y_test_pred_threshold == 1) & (y_val == 0))\n",
    "    test_fn = np.sum((y_test_pred_threshold == 0) & (y_val == 1))\n",
    "    test_precision = test_tp / (test_tp + test_fp) if (test_tp + test_fp) > 0 else 0\n",
    "    test_recall = test_tp / (test_tp + test_fn) if (test_tp + test_fn) > 0 else 0\n",
    "    test_f1 = 2 * test_precision * test_recall / (test_precision + test_recall) if (test_precision + test_recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nThreshold = {threshold}\")\n",
    "    print(f\"Train: Precision = {train_precision:.4f}, Recall = {train_recall:.4f}, F1 = {train_f1:.4f}\")\n",
    "    print(f\"Test: Precision = {test_precision:.4f}, Recall = {test_recall:.4f}, F1 = {test_f1:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix for test set\n",
    "''' plt.figure(figsize=(8, 6))\n",
    "    cm = np.zeros((2, 2))\n",
    "    cm[0, 0] = np.sum((y_test_pred_threshold == 0) & (y_test == 0))  # TN\n",
    "    cm[0, 1] = test_fn  # FN\n",
    "    cm[1, 0] = test_fp  # FP\n",
    "    cm[1, 1] = test_tp  # TP\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix (Threshold = {threshold})', fontsize=16)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['Not Churned (0)', 'Churned (1)']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, f\"{int(cm[i, j])}\", \n",
    "                     horizontalalignment=\"center\", \n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3b96b-8d1b-433f-87db-74d9daceac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(X, y, n_folds=5, lambda_values=None, learning_rate=0.5, iterations=10000):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation to evaluate model performance with different lambda values\n",
    "    \n",
    "    Parameters:\n",
    "    X: Features DataFrame\n",
    "    y: Target variable\n",
    "    n_folds: Number of CV folds\n",
    "    lambda_values: List of lambda values to test\n",
    "    learning_rate: Learning rate for gradient descent\n",
    "    iterations: Number of iterations for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary with results for each lambda value\n",
    "    \"\"\"\n",
    "    if lambda_values is None:\n",
    "        lambda_values = [100.0,130,140 , 145 ,150, 155,  160.0 , 165 , 170 , 175 , 180 , 185 , 190 ,195 ,  200]\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {lamb: {'train_accuracy': [], 'val_accuracy': [], \n",
    "                      'train_precision': [], 'val_precision': [],\n",
    "                      'train_recall': [], 'val_recall': [],\n",
    "                      'train_f1': [], 'val_f1': []} \n",
    "              for lamb in lambda_values}\n",
    "    \n",
    "    # Create indices for k-fold cross-validation\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    fold_size = n_samples // n_folds\n",
    "    folds = [indices[i*fold_size:(i+1)*fold_size] for i in range(n_folds)]\n",
    "    \n",
    "    # For the last fold, include any remaining samples\n",
    "    if n_samples % n_folds != 0:\n",
    "        folds[-1] = np.concatenate([folds[-1], indices[n_folds*fold_size:]])\n",
    "    \n",
    "    # Start cross-validation\n",
    "    for fold_idx, val_indices in enumerate(folds):\n",
    "        print(f\"\\nProcessing fold {fold_idx+1}/{n_folds}\")\n",
    "        \n",
    "        # Split data into training and validation\n",
    "        train_indices = np.concatenate([folds[i] for i in range(n_folds) if i != fold_idx])\n",
    "        \n",
    "        X_train_fold = X.iloc[train_indices]\n",
    "        y_train_fold = y.iloc[train_indices]\n",
    "        X_val_fold = X.iloc[val_indices]\n",
    "        y_val_fold = y.iloc[val_indices]\n",
    "        \n",
    "        # Engineer features\n",
    "        #X_train_fold = engineer_features(X_train_fold)\n",
    "        #X_val_fold = engineer_features(X_val_fold)\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled, scaling_params = feature_scaling(X_train_fold, return_params=True)\n",
    "        \n",
    "        # Apply the same scaling to validation set\n",
    "        X_val_scaled = X_val_fold.copy().astype('float')\n",
    "        for col in ['tenure', 'MonthlyCharges', 'TotalCharges', 'tenure_month_to_month',\n",
    "                   'charge_per_tenure', 'avg_monthly_spend', 'tenure_one_year', \n",
    "                   'tenure_two_year', 'recent_vs_lifetime_spend', 'total_services', 'loyalty_score']:\n",
    "            if col in scaling_params['mean'] and col in X_val_scaled.columns:\n",
    "                X_val_scaled[col] = (X_val_fold[col] - scaling_params['mean'][col]) / scaling_params['std'][col]\n",
    "        \n",
    "        # Define noisy columns to remove\n",
    "        noisy_cols = ['StreamingTV_No internet service', 'DeviceProtection_No internet service', \n",
    "         'TechSupport_No internet service', 'StreamingMovies_No internet service', 'InternetService_No', 'OnlineBackup_No internet service',\n",
    "         'OnlineSecurity_No internet service', 'Dependents_Yes', 'DeviceProtection_Yes', 'tenure_one_year', 'loyalty_score', \n",
    "         'PaymentMethod_Credit card (automatic)', 'Partner_Yes',\n",
    "         'PaymentMethod_Mailed check', 'avg_monthly_spend', 'recent_vs_lifetime_spend', 'tenure_two_year']\n",
    "             \n",
    "        # Remove noisy columns\n",
    "        X_train_clean = X_train_scaled.drop(columns=noisy_cols, errors='ignore')\n",
    "        X_val_clean = X_val_scaled.drop(columns=noisy_cols, errors='ignore')\n",
    "        \n",
    "        # Evaluate each lambda value\n",
    "        for lamb in lambda_values:\n",
    "            print(f\"  Testing lambda={lamb}\")\n",
    "            \n",
    "            # Initialize weights and bias\n",
    "            m, n = X_train_clean.shape\n",
    "            w = np.zeros(n)\n",
    "            b = 0\n",
    "            \n",
    "            # Train model\n",
    "            w, b, costs, iterations_list = gradient_descent_with_regularization(\n",
    "                X_train_clean, y_train_fold, w, b, learning_rate, iterations, lamb)\n",
    "            \n",
    "            # Evaluate on training set\n",
    "            train_accuracy, train_precision, train_recall, train_f1 = evaluate_classification_model(\n",
    "                X_train_clean, y_train_fold, w, b)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_accuracy, val_precision, val_recall, val_f1 = evaluate_classification_model(\n",
    "                X_val_clean, y_val_fold, w, b)\n",
    "            \n",
    "            # Store results\n",
    "            results[lamb]['train_accuracy'].append(train_accuracy)\n",
    "            results[lamb]['val_accuracy'].append(val_accuracy)\n",
    "            results[lamb]['train_precision'].append(train_precision)\n",
    "            results[lamb]['val_precision'].append(val_precision)\n",
    "            results[lamb]['train_recall'].append(train_recall)\n",
    "            results[lamb]['val_recall'].append(val_recall)\n",
    "            results[lamb]['train_f1'].append(train_f1)\n",
    "            results[lamb]['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Calculate average metrics across folds\n",
    "    for lamb in lambda_values:\n",
    "        original_metrics = list(results[lamb].keys())  # Create a copy of the keys\n",
    "        for metric in original_metrics:                # Iterate over the copy\n",
    "            results[lamb][f'avg_{metric}'] = np.mean(results[lamb][metric])\n",
    "            results[lamb][f'std_{metric}'] = np.std(results[lamb][metric])    \n",
    "    return results\n",
    "\n",
    "def visualize_cv_results(cv_results):\n",
    "    \"\"\"\n",
    "    Visualize cross-validation results\n",
    "    \n",
    "    Parameters:\n",
    "    cv_results: Dictionary with cross-validation results\n",
    "    \"\"\"\n",
    "    lambda_values = list(cv_results.keys())\n",
    "    \n",
    "    # Plot F1 scores\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    \n",
    "    train_f1_means = [cv_results[lamb]['avg_train_f1'] for lamb in lambda_values]\n",
    "    val_f1_means = [cv_results[lamb]['avg_val_f1'] for lamb in lambda_values]\n",
    "    val_f1_stds = [cv_results[lamb]['std_val_f1'] for lamb in lambda_values]\n",
    "    \n",
    "    plt.errorbar(lambda_values, val_f1_means, yerr=val_f1_stds, fmt='o-', label='Validation F1')\n",
    "    plt.plot(lambda_values, train_f1_means, 'x-', label='Training F1')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Lambda (Regularization Parameter)')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs Lambda')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    \n",
    "    train_acc_means = [cv_results[lamb]['avg_train_accuracy'] for lamb in lambda_values]\n",
    "    val_acc_means = [cv_results[lamb]['avg_val_accuracy'] for lamb in lambda_values]\n",
    "    val_acc_stds = [cv_results[lamb]['std_val_accuracy'] for lamb in lambda_values]\n",
    "    \n",
    "    plt.errorbar(lambda_values, val_acc_means, yerr=val_acc_stds, fmt='o-', label='Validation Accuracy')\n",
    "    plt.plot(lambda_values, train_acc_means, 'x-', label='Training Accuracy')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Lambda (Regularization Parameter)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Lambda')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Precision\n",
    "    plt.subplot(2, 2, 3)\n",
    "    \n",
    "    train_precision_means = [cv_results[lamb]['avg_train_precision'] for lamb in lambda_values]\n",
    "    val_precision_means = [cv_results[lamb]['avg_val_precision'] for lamb in lambda_values]\n",
    "    val_precision_stds = [cv_results[lamb]['std_val_precision'] for lamb in lambda_values]\n",
    "    \n",
    "    plt.errorbar(lambda_values, val_precision_means, yerr=val_precision_stds, fmt='o-', label='Validation Precision')\n",
    "    plt.plot(lambda_values, train_precision_means, 'x-', label='Training Precision')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Lambda (Regularization Parameter)')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision vs Lambda')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Recall\n",
    "    plt.subplot(2, 2, 4)\n",
    "    \n",
    "    train_recall_means = [cv_results[lamb]['avg_train_recall'] for lamb in lambda_values]\n",
    "    val_recall_means = [cv_results[lamb]['avg_val_recall'] for lamb in lambda_values]\n",
    "    val_recall_stds = [cv_results[lamb]['std_val_recall'] for lamb in lambda_values]\n",
    "    \n",
    "    plt.errorbar(lambda_values, val_recall_means, yerr=val_recall_stds, fmt='o-', label='Validation Recall')\n",
    "    plt.plot(lambda_values, train_recall_means, 'x-', label='Training Recall')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Lambda (Regularization Parameter)')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Recall vs Lambda')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best lambda based on validation F1 score\n",
    "    best_lambda = lambda_values[np.argmax(val_f1_means)]\n",
    "    print(f\"\\nBest lambda based on validation F1 score: {best_lambda}\")\n",
    "    \n",
    "    # Print summary table of results\n",
    "    print(\"\\nSummary of Cross-Validation Results:\")\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Lambda': lambda_values,\n",
    "        'Training F1': train_f1_means,\n",
    "        'Validation F1': val_f1_means,\n",
    "        'Validation F1 Std': val_f1_stds,\n",
    "        'Training Accuracy': train_acc_means,\n",
    "        'Validation Accuracy': val_acc_means,\n",
    "        'Validation Accuracy Std': val_acc_stds,\n",
    "        'Recall Accuracy': val_recall_means,\n",
    "        'Recall Accuracy Std': val_recall_stds,\n",
    "        'Precision Accuracy': val_precision_means,\n",
    "        'Precision Accuracy Std': val_precision_stds,\n",
    "    })\n",
    "    print(summary_df.round(4))\n",
    "\n",
    "# Example usage\n",
    "lambda_values = [60, 70 , 80 , 90 , 100.0, 130, 150,  160.0  , 180 ,190 ,  200, 220, 240, 280 , 300 , 340  ]\n",
    "    \n",
    "X = final_df.drop('Churn', axis=1)\n",
    "y = final_df['Churn']\n",
    "print(X.columns)\n",
    "# Run cross-validation with multiple lambda values\n",
    "cv_results = cross_validate_model(X, y, n_folds=5, lambda_values=lambda_values, \n",
    "                                 learning_rate=0.5, iterations=10000)\n",
    "\n",
    "# Visualize results\n",
    "visualize_cv_results(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b47a43-9a91-4143-a4bb-5c0fef07cbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864fd74f-b918-4195-97d1-2391cd5862cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
